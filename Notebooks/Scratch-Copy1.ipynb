{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from itertools import izip\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from gensim.parsing import preprocessing as gprocessing\n",
    "from spacy.en import English as nlp\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/Storage/Coding_Projects/Candidate_Classifier'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from candidate_classifier.nltk_model import NgramModel\n",
    "from candidate_classifier import utils\n",
    "from nltk.probability import LaplaceProbDist, LidstoneProbDist\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import switchboard, nps_chat\n",
    "from nltk.corpus.reader.util import *\n",
    "from nltk.tokenize import *\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_root = os.path.abspath('candidate_classifier/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debates = DebateCorpusReader('candidate_classifier/data', '.*', word_tokenizer=DummyTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from candidate_classifier.nltk_model import NgramModel\n",
    "from nltk.probability import LaplaceProbDist, LidstoneProbDist, SimpleGoodTuringProbDist\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "est = lambda freqdist, bins: LidstoneProbDist(freqdist, 0.2, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e2 = lambda freqdist, bins: SimpleGoodTuringProbDist(freqdist, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"This is a test.\".split()\n",
    "s2 = \"I'm starting to know what God felt like when he sat out there in the darkness, creating the world.\".split()\n",
    "\n",
    "text = (s for s in [s1, s2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "bins parameter must not be less than 23=freqdist.B()+1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b40d764f709a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNgramModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/mnt/Storage/Coding_Projects/Candidate_Classifier/candidate_classifier/nltk_model/ngram.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n, train, pad_left, pad_right, estimator, **estimator_kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m                                         \u001b[0mpad_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_right\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                                         \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                                         **estimator_kwargs)\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backoff_alphas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/mnt/Storage/Coding_Projects/Candidate_Classifier/candidate_classifier/nltk_model/ngram.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n, train, pad_left, pad_right, estimator, **estimator_kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m                                         \u001b[0mpad_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_right\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                                         \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                                         **estimator_kwargs)\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backoff_alphas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/mnt/Storage/Coding_Projects/Candidate_Classifier/candidate_classifier/nltk_model/ngram.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n, train, pad_left, pad_right, estimator, **estimator_kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mestimator_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bins'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConditionalProbDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mestimator_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# recursively construct the lower-order models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/probability.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cfdist, probdist_factory, *factory_args, **factory_kw_args)\u001b[0m\n\u001b[0;32m   1941\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcondition\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcfdist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1942\u001b[0m             self[condition] = probdist_factory(cfdist[condition],\n\u001b[1;32m-> 1943\u001b[1;33m                                                *factory_args, **factory_kw_args)\n\u001b[0m\u001b[0;32m   1944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1945\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-685ccc317fd5>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(freqdist, bins)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0me2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSimpleGoodTuringProbDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreqdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/probability.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, freqdist, bins)\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \"\"\"\n\u001b[0;32m   1217\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mbins\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mbins\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m                \u001b[1;34m'bins parameter must not be less than %d=freqdist.B()+1'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1219\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbins\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m             \u001b[0mbins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfreqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: bins parameter must not be less than 23=freqdist.B()+1"
     ]
    }
   ],
   "source": [
    "model = NgramModel(3, text, estimator=e2, bins=len(set(flatten([s1, s2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'.', u'.', u'.', u'.', u'.', u'.', u'.', u'.', u'.', u'.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for k, v in debates.joined_speaker_paras().iteritems():\n",
    "    if stats[k]['num_word_types'] >= 100:\n",
    "#         docs = (concat(chunk) for chunk in itertools.islice((sent.split() for sent in flatten(v)), 10))\n",
    "        docs = (' '.join(chunk).split() for chunk in chunker(flatten(v)))\n",
    "#         print list(docs)[:2]\n",
    "        models[k] = NgramModel(3, docs, estimator=est, bins=stats[k]['num_word_types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> so, look, we have a tremendous success. </S> <S> well, i'd like to respond, i'd like to tell you, we are spending $200 billion a year imbalance. </S> <S> assad is fighting isis. </S> <S> we cannot lose this election. </S> <S> but i say that i have used \n",
      "\n",
      "<S> second of all, let me — caesar's, the rolls-royce, as you too know, i know i built a very good people are going to find a finer men. </S> <S> but a lot — but the democrats are going to even say — i think, for me, nuclear is \n",
      "\n",
      "<S> i agree 100 percent, dana. </S> <S> mr. trump\" — these are terrible people in actually 15 — going to have people come into our country has right now by people that want to endorse me. </S> <S> you feel safe right now? </S> <S> i spoke to a \n",
      "\n",
      "<S> it's hurting us from every standpoint. </S> <S> are you going to embarrass, but virtually every person that you wouldn't have millions of jobs and manufacturing back. </S> <S> one thing at a time. </S> <S> so we can get the senate, but i hope that answers your question. \n",
      "\n",
      "<S> so when nikki said that, i wasn't a sitting politician going into iraq, because it was my tone. </S> <S> in order to get involved with china, they tax us. </S> <S> they had it down at three, three and a half, including them, and i am discussing it \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print ' '.join(models['TRUMP'].generate(50)), '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> if they had — they are barrel bombing the innocents in that country. </S> <S> if we're serious about high growth, then we 'ought to create more government from the federal law. </S> <S> when i'm elected president, and hillary clinton, who can't even project force. </S> <S> to \n",
      "\n",
      "<S> yeah. here's — apart from that, there will be — this is recognizing that there will be — this is not a factor. </S> <S> everybody's record's going to win when we — you flew in with your 767, didn't you? </S> <S> imagine what it's like now to \n",
      "\n",
      "<S> it's a good job. </S> <S> that expertise needs to complete dialogue with us. </S> <S> so he supports clinton — — the pacific agreement. </S> <S> lay them out there. </S> <S> i just did. </S> <S> i look forward to talking tonight about how bad washington, d.c. \n",
      "\n",
      "<S> this would be devastating for agriculture and many of the hhs funding, there is a place where security can take place. </S> <S> 6.5 million people are suffering today in america. </S> <S> no it isn't because of the central planners in washington d.c. </S> <S> but she wants \n",
      "\n",
      "<S> the congress has funded these programs of building more fencing and we won. </S> <S> no one goes below. </S> <S> but she wants a secure border. </S> <S> so, here's the deal. </S> <S> the b-52 is still the most talented guy in the aftermath of the world \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print ' '.join(models['BUSH'].generate(50)), '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'<S>',\n",
       "  u\"that's\",\n",
       "  u\"what's\",\n",
       "  u'wrong',\n",
       "  u'with',\n",
       "  u'politicians',\n",
       "  u'in',\n",
       "  u'washington',\n",
       "  u'right',\n",
       "  u'now.',\n",
       "  u'</S>'],\n",
       " [u'<S>',\n",
       "  u'they',\n",
       "  u'think',\n",
       "  u'we',\n",
       "  u'can',\n",
       "  u'take',\n",
       "  u'a',\n",
       "  u'country',\n",
       "  u'into',\n",
       "  u'bankruptcy.',\n",
       "  u'</S>']]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent.split() for sent in flatten(debates.joined_speaker_paras()['???'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c(b(a(doc)))'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: '{}({})'.format(y, x), ['a', 'b', 'c'], 'doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rrun(s):\n",
    "    # base case\n",
    "    if issubclass(type(s), basestring):\n",
    "        yield s\n",
    "    # Recursive case\n",
    "    else:\n",
    "        for subelt in s:\n",
    "            for elt in rrun(subelt):\n",
    "                yield elt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '6']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [['1','2'], ['3', '4'], '5', '6']\n",
    "\n",
    "[s for s in rrun(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foo', 'bar', 'baz']\n"
     ]
    }
   ],
   "source": [
    "def flatten(nested):\n",
    "    try:\n",
    "        try: nested + ''\n",
    "        except TypeError: pass\n",
    "        else: raise TypeError\n",
    "        for sublist in nested:\n",
    "            for element in flatten(sublist):\n",
    "                yield element\n",
    "    except TypeError:\n",
    "        yield nested\n",
    "        \n",
    "print list(flatten(['foo', ['bar', ['baz']]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(flatten([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 9, 8]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in flatten([((j for j in xrange(i)) for i in xrange(5)), 9, 8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nested_map(root, func):\n",
    "    \"\"\"Takes in an arbitrarily nested iterator and traverses it\n",
    "     deapth first to create generators at every level, where\n",
    "     every element has a function applied to it.\n",
    "     \"\"\"\n",
    "    try:\n",
    "        # Don't unwind strings\n",
    "        try:\n",
    "            root + ''\n",
    "        except TypeError:\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError\n",
    "        \n",
    "        return (nested_map(elt, func) for elt in root)\n",
    "        \n",
    "    except TypeError:\n",
    "        return func(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<generator object <genexpr> at 0x7f20b18c0230>,\n",
       " <generator object <genexpr> at 0x7f20b18c0fa0>,\n",
       " 5,\n",
       " 6]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in nested_map(test, int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python2\n",
    "\n",
    "\"\"\"\n",
    "Idea behind the structure:\n",
    "\n",
    " String Level Transformations\n",
    "   - Encodings\n",
    "   - Substitutions\n",
    " Tokenize\n",
    " Sentence Level Transformations\n",
    "   - Filters\n",
    "   - Transformations\n",
    " Token Level Transformations\n",
    "   - Filters\n",
    "   - Substitutions\n",
    "   - Additions\n",
    "       - Append POS, lemmatize, etc\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import string\n",
    "# import itertools\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from gensim.utils import any2unicode, any2utf8, decode_htmlentities, deaccent\n",
    "from gensim.parsing import preprocessing as gprocessing\n",
    "from candidate_classifier import utils\n",
    "\n",
    "\n",
    "__author__ = 'Eric Lind'\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# pre/postfilter substitutions\n",
    "#  - What if the substitution you perform changes the string so that it would have been filtered?\n",
    "\n",
    "\n",
    "class StringProcessor(object):\n",
    "    def __init__(self,\n",
    "                 string_transformer,\n",
    "                 sentence_transformer=None,\n",
    "                 token_transformer=None):\n",
    "        \"\"\"Autobots, roll out!\"\"\"\n",
    "        self.str_transformer = string_transformer\n",
    "        self.sent_transformer = sentence_transformer\n",
    "        self.tok_transformer = token_transformer\n",
    "\n",
    "    def __call__(self, s):\n",
    "        # Process string\n",
    "        processed = self.str_transformer(s)\n",
    "\n",
    "        # TODO: Fix this and figure our contract\n",
    "        # Sentences\n",
    "        if self.sent_transformer is not None:\n",
    "            sents = self.sent_transformer(s)\n",
    "\n",
    "            for sent in sents:\n",
    "                # Tokens\n",
    "                if self.tok_transformer is not None:\n",
    "                    sent = self.tok_transformer(sent)\n",
    "\n",
    "                yield sent\n",
    "\n",
    "\n",
    "\n",
    "    # def __call__(self, s):\n",
    "    #     Yes, I know this is difficult to read.  But this is\n",
    "    #     my library and I'll do as I please thank you very much.\n",
    "    #     The contract of this reduce operation is essentially this:\n",
    "    #     f([a, b, c], 's') ==> c(b(a('s')))\n",
    "    #     The reason for the reversal is so that when a user specifies\n",
    "    #     the function they want to call first, it can be first in the list.\n",
    "        # return reduce(lambda x, y: y(x), self.transformers, s)\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# Make a repr that shows the current filters/substitutions being used\n",
    "# Add the available built-in filters/substitutions to the docstring\n",
    "class TransformerABC(object):\n",
    "    \"\"\"Interface for applying transformations to a string/token\"\"\"\n",
    "\n",
    "    re_type = type(re.compile(r''))\n",
    "    ws_pattern = re.compile(r\"white(?:-|_|\\s)?space\", re.I)\n",
    "    non_ascii_pattern = re.compile(r\"non(?:-|_|\\s)?ascii\", re.I)\n",
    "    h_ents_pattern = re.compile(r\"html(?:-|_|\\s)?entities\", re.I)\n",
    "    # TODO: Make min_len an optional argument\n",
    "    min_len = 3\n",
    "\n",
    "    def __init__(self,\n",
    "                 prefilter_substitutions=(),\n",
    "                 postfilter_substitutions=(),\n",
    "                 filters=(),\n",
    "                 normalize_encoding=True,\n",
    "                 flatten=True,  # TODO: Document\n",
    "                 # Tokenization always occurs last.\n",
    "                 # If you want to process the tokens, use another Transformer\n",
    "                 # tokenizer must be callable\n",
    "                 tokenizer=None,\n",
    "                 **kwargs):\n",
    "        self.filters = self._process_filters(filters)\n",
    "        self.pre_substitutions = self._process_substitutions(prefilter_substitutions)\n",
    "        self.post_substitutions = self._process_substitutions(postfilter_substitutions)\n",
    "        self.normalize_encoding = normalize_encoding\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Set processor\n",
    "        self.processor = self._flat_process if flatten else self._nested_process\n",
    "\n",
    "\n",
    "    def __call__(self, s):\n",
    "        # FIXME More documnetation for empty string behavior\n",
    "        return (i for i in self.processor(s) if i)\n",
    "\n",
    "\n",
    "    def _process_filters(self, filts):\n",
    "        \"\"\"\n",
    "        Filters are functions that return booleans.  If a function\n",
    "        resturns True (or some truthy value) for a given string,\n",
    "        that string is filtered out.\n",
    "        \"\"\"\n",
    "        t = []\n",
    "\n",
    "        for f in filts:\n",
    "            # Length\n",
    "            if f in {'length', 'len'}:\n",
    "                # Default length is 3\n",
    "                # Create a closure\n",
    "                def funcC(fil):\n",
    "                    def func(s): return len(s) <= self.min_len\n",
    "                    return func\n",
    "                t.append(funcC(f))\n",
    "                # t.append(lambda s: len(s) <= self.min_len)\n",
    "            elif hasattr(f, '__getitem__'):\n",
    "                try:\n",
    "                    if f[0] in {'length', 'len', 'short'}:\n",
    "                        t.append(lambda s: len(s) <= s[1])\n",
    "                # FIXME: More thorough checks\n",
    "                except (IndexError, TypeError):\n",
    "                    pass\n",
    "\n",
    "            # Patterns\n",
    "            elif isinstance(f, self.re_type):\n",
    "                # Create a closure for the current filter\n",
    "                def funcC(fil):\n",
    "                    def func(s): return re.search(fil, s)\n",
    "                    return func\n",
    "                t.append(funcC(f))\n",
    "                # t.append(lambda s: re.search(f, s))\n",
    "\n",
    "            # Non-Ascii\n",
    "            elif re.search(self.non_ascii_pattern, str(f)):\n",
    "                t.append(self._is_non_ascii)\n",
    "\n",
    "            # Callable\n",
    "            elif hasattr(f, '__call__'):\n",
    "                t.append(f)\n",
    "\n",
    "            # TODO:\n",
    "            # html tags\n",
    "            # punctuation\n",
    "\n",
    "        return t\n",
    "\n",
    "\n",
    "    def _process_substitutions(self, subs):\n",
    "        t = []\n",
    "\n",
    "        # Go through all subs and prepare/apply them.\n",
    "        # The order is IMPORTANT.  It is expected that\n",
    "        # the user knows what order they want!\n",
    "        for sub in subs:\n",
    "            # HTML\n",
    "            if sub == 'html':\n",
    "                t.append(lambda s: bs(s, 'lxml').get_text())\n",
    "\n",
    "            # HTML entities\n",
    "            # if sub in {'htmlentities', 'html entities', 'html_entities', 'html-entities'}:\n",
    "            elif re.search(self.h_ents_pattern, str(sub)):\n",
    "                t.append(decode_htmlentities)\n",
    "\n",
    "            # Daccent\n",
    "            elif sub == 'deaccent':\n",
    "                t.append(deaccent)\n",
    "\n",
    "            # Punctuation\n",
    "            elif sub in {'punct', 'punctuation', 'puncts'}:\n",
    "                t.append(gprocessing.strip_punctuation)\n",
    "\n",
    "            # Case\n",
    "            elif sub == 'lower':\n",
    "                t.append(string.lower)\n",
    "            elif sub == 'upper':\n",
    "                t.append(string.upper)\n",
    "\n",
    "            # Whitespace\n",
    "            elif re.search(self.ws_pattern, str(sub)):\n",
    "                t.append(gprocessing.strip_multiple_whitespaces)\n",
    "            elif sub == 'strip':\n",
    "                t.append(string.strip)\n",
    "\n",
    "            # callable\n",
    "            elif hasattr(sub, '__call__'):\n",
    "                t.append(sub)\n",
    "\n",
    "            # patterns\n",
    "            elif isinstance(sub, self.re_type):\n",
    "                # TODO: Make more elegant\n",
    "                def funcC(sub):\n",
    "                    def func(s): return sub.sub('', s)\n",
    "                    return func\n",
    "                t.append(funcC(sub))\n",
    "                # t.append(lambda s: sub.sub('', s))\n",
    "            else:\n",
    "                try:\n",
    "                    if isinstance(sub[0], self.re_type):\n",
    "                        def funcC(sub):\n",
    "                            # FIXME: Use compiled pattern\n",
    "                            # def func(s): sub[0].sub(sub[1], s)\n",
    "                            def func(s): return re.sub(sub[0], sub[1], s)\n",
    "                            return func\n",
    "                        t.append(funcC(sub))\n",
    "                        # t.append(lambda s: re.sub(sub[0], sub[1], s))\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "        return t\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_non_ascii(s):\n",
    "        \"\"\"Returns True if the string contains non-ascii characters\n",
    "        Fastest way to check for non-ascii:\n",
    "        http://stackoverflow.com/questions/196345/how-to-check-if-a-string-in-python-is-in-ascii\n",
    "        \"\"\"\n",
    "        try:\n",
    "            s.decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _flat_process(self, s):\n",
    "        \"\"\"Takes in either an iterator or a string and yields strings.\n",
    "        Calling this on a nested list will result in a flattened list.\n",
    "        \"\"\"\n",
    "        for elt in utils.flatten(s):\n",
    "            # print elt\n",
    "            # Tokenization may add another level of nesting\n",
    "            for t in utils.flatten(self._process(elt)):\n",
    "                # print t\n",
    "                yield t\n",
    "\n",
    "    def _nested_process(self, s):\n",
    "        return utils.nested_map(s, self._process)\n",
    "\n",
    "    def _process(self, s):\n",
    "        # TODO: Clarify contract return value for filtered strings\n",
    "        # TODO: Add docs for: if a filter evaluates as False the string is discarded\n",
    "\n",
    "        # Skip empty strings:\n",
    "        if len(s) == 0:\n",
    "            return s\n",
    "\n",
    "        # Normalize the encoding before anything else\n",
    "        if self.normalize_encoding:\n",
    "            s = any2unicode(s)\n",
    "\n",
    "        # Apply sub, filter, sub\n",
    "        for sub in self.pre_substitutions:\n",
    "            s = sub(s)\n",
    "\n",
    "        # If any of the filters return True, filter the string\n",
    "        if any(f(s) for f in self.filters):\n",
    "            return ''\n",
    "\n",
    "        for sub in self.post_substitutions:\n",
    "            s = sub(s)\n",
    "\n",
    "        # Tokenize last.  If you want to process the tokens use another\n",
    "        # Transformer\n",
    "        if self.tokenizer:\n",
    "            s = self.tokenizer(s)\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class StringTransformer(TransformerABC):\n",
    "    def __init__(self,\n",
    "                 prefilter_substitutions=(),\n",
    "                 postfilter_substitutions=(),\n",
    "                 filters=()):\n",
    "        super(StringTransformer, self).__init__(prefilter_substitutions,\n",
    "                                                postfilter_substitutions,\n",
    "                                                filters)\n",
    "\n",
    "    def __call__(self, s):\n",
    "        # Normalize encoding\n",
    "        # TODO: Find a more robust function for this (e.g. from another library)\n",
    "        s = any2unicode(s)\n",
    "\n",
    "        return self._process(s)\n",
    "\n",
    "\n",
    "\n",
    "class SentenceTransformer(TransformerABC):\n",
    "    def __init__(self,\n",
    "                 prefilter_substitutions=(),\n",
    "                 postfilter_substitutions=(),\n",
    "                 filters=(),\n",
    "                 sent_tokenizer=None):\n",
    "        super(SentenceTransformer, self).__init__(prefilter_substitutions,\n",
    "                                                  postfilter_substitutions,\n",
    "                                                  filters)\n",
    "        # If no tokenizer is specified, it is assumed that\n",
    "        # this will be called on a pre-tokenized list of sentences\n",
    "        self.tokenizer = sent_tokenizer\n",
    "\n",
    "    def __call__(self, s):\n",
    "        \"\"\"Takes in a string and yields sentences\"\"\"\n",
    "        if self.tokenizer is not None:\n",
    "            sents = self.tokenizer(s)\n",
    "        else:\n",
    "            sents = s\n",
    "\n",
    "        for sent in sents:\n",
    "            yield self._process(sent)\n",
    "\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# - Stopwords\n",
    "# - Punctuation\n",
    "# is non-ascii\n",
    "# is numeric\n",
    "# is url\n",
    "# is email address\n",
    "class TokenTransformer(TransformerABC):\n",
    "    def __init__(self,\n",
    "                 prefilter_substitutions=(),\n",
    "                 postfilter_substitutions=(),\n",
    "                 filters=(),\n",
    "                 tokenizer=None):\n",
    "        super(TokenTransformer, self).__init__(prefilter_substitutions,\n",
    "                                               postfilter_substitutions,\n",
    "                                               filters)\n",
    "        # self.filters.extend(self._process_token_filters(filters))\n",
    "        # self.pre_substitutions.extend(self._process_token_substitutions(prefilter_substitutions))\n",
    "        # self.post_substitutions.extend(self._process_token_substitutions(postfilter_substitutions))\n",
    "\n",
    "        # If no tokenizer is specified, it is assumed that\n",
    "        # this will be called on a pre-tokenized list of words\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _process_token_filters(self, filters):\n",
    "        # Stopwords\n",
    "        # if f in {'stop', 'stops', 'stopwords'}:\n",
    "        #     pass\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _process_token_substitutions(self, substitutions):\n",
    "        pass\n",
    "\n",
    "    # def __call__(self, sent):\n",
    "    #     \"\"\"Takes in a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
