{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/Storage/Coding_Projects/Candidate_Classifier'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/Storage/Coding_Projects/Candidate_Classifier'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from candidate_classifier.nltk_model import NgramModel\n",
    "from candidate_classifier import utils\n",
    "from nltk.probability import LaplaceProbDist, LidstoneProbDist\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os\n",
    "from candidate_classifier.debate_corpus_reader import DebateCorpusReader\n",
    "from candidate_classifier.string_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = English(entity=False, load_vectors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransformerWrapper(object):\n",
    "    def __init__(self, transformer):\n",
    "        self.transformer = transformer\n",
    "    \n",
    "    def tokenize(self, s):\n",
    "        return self.transformer(s)\n",
    "\n",
    "class DummyTokenizer(object):\n",
    "    def tokenize(self, s):\n",
    "        return s\n",
    "    \n",
    "def sent_tokenizer(s):\n",
    "    doc = nlp(s)\n",
    "    return [u''.join(t.text_with_ws for t in sent) for sent in doc.sents]\n",
    "\n",
    "\n",
    "# def word_tokenizer(s):\n",
    "#     toks = nlp(s)\n",
    "#     return ['<S>'] + [t.lower_ for t in toks] + ['</S>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace [*] with ''\n",
    "# Replace '. . .' with '...'\n",
    "# Replace multiple ellipses with single ...\n",
    "# Remove all sentences that end with ...\n",
    "\n",
    "BRACKET_PATTERN = re.compile(r\"\\[[a-zA-Z ]*\\]\", re.U)\n",
    "SPACED_ELLIPSIS_PATTERN = re.compile(r\"((?:\\.\\s){3})\")\n",
    "MULTI_ELLIPSIS_PATTERN = re.compile(r\"(?:(?:\\.){3} ?)+\")\n",
    "ELLIPSIS_BRACKET_PATTERN = re.compile(r\"(?:(?:\\.){3}) *\\[[a-zA-Z ]*\\] *(?:(?:\\.){3} ?)\")\n",
    "ENDS_WITH_ELLIPSIS = lambda s: s[-3:] == '...'\n",
    "STARTS_WITH_ELLIPSIS = lambda s: s[:3] == '...'\n",
    "STARTS_WITH_DASH = lambda s: s[0] == '-'\n",
    "ENDS_WITH_DASH = lambda s: s[-1] == '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_transformer = TransformerABC(\n",
    "    prefilter_substitutions=['html entities',\n",
    "                             'deaccent',\n",
    "                             'whitespace',\n",
    "                             (ELLIPSIS_BRACKET_PATTERN, ' '),\n",
    "                             BRACKET_PATTERN,\n",
    "                             (SPACED_ELLIPSIS_PATTERN, '...'),\n",
    "                             (MULTI_ELLIPSIS_PATTERN, '...'),\n",
    "                             'whitespace',\n",
    "                             'strip'],\n",
    "    tokenizer=sent_tokenizer)\n",
    "\n",
    "sent_filter = TransformerABC(\n",
    "    prefilter_substitutions=['puntuation', 'strip'],\n",
    "    filters=[('len', 49)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcr = DebateCorpusReader('candidate_classifier/data/raw', '.*\\.txt', \n",
    "                             sent_tokenizer=TransformerWrapper(doc_transformer), \n",
    "                             word_tokenizer=DummyTokenizer())\n",
    "candidates = ['BUSH', 'CARSON', 'CHRISTIE', 'CRUZ', 'KASICH', 'RUBIO', 'TRUMP', 'CLINTON', 'SANDERS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess to sentences\n",
    "I have officially decided that the task is classifying sentences and snippits.  As such, each document will be a sentence.  \n",
    "\n",
    "#### Precrocessing Steps\n",
    "- Tokenize to sentences and normalize encoding\n",
    "- remove all non-ascii characters\n",
    "- All whitespace to spaces (no newlines)\n",
    "- Filter all sentences that are less than 50 characters (after removing all punctuation)\n",
    "- Group all sentences together with their labels and shuffle them\n",
    "- Write text and labels to (separate) line-delimited files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_sents = []\n",
    "for label, sents in dcr.grouped_sents(speakers=candidates).iteritems():\n",
    "    for sent in sents:\n",
    "#             stripped = sent.strip(string.punctuation+ ' \\n\\t')\n",
    "#             if len(stripped) >= 35:\n",
    "#                 cleaned_sents.append((label, sent))\n",
    "        if sent_filter(sent):\n",
    "            cleaned_sents.append((label, sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8851"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(cleaned_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_path = 'candidate_classifier/data/processed/clean_sents.txt'\n",
    "labels_path = 'candidate_classifier/data/processed/sent_labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "sents_file = codecs.open(sents_path, mode='w', encoding='utf-8')\n",
    "labels_file = codecs.open(labels_path, mode='w', encoding='utf-8')\n",
    "    \n",
    "for sent in cleaned_sents:\n",
    "    sents_file.write(u'%s\\n' % sent[1])\n",
    "    labels_file.write(u'%s\\n' % sent[0])\n",
    "\n",
    "sents_file.close()\n",
    "labels_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_stats(dcr, filt, speakers=None):\n",
    "    \"\"\"\n",
    "    Returns a dictionary for the given speakers with stats for all sentences\n",
    "    dcr is a DebateCorpusReader\n",
    "    filt is a callable that will turn sentences we want to filter into a falsy value\n",
    "    \"\"\"\n",
    "    if speakers is None:\n",
    "        speakers = dcr.speakers()\n",
    "    stats = {s:dict() for s in speakers}\n",
    "    \n",
    "    for speaker, sents in dcr.grouped_sents(speakers=speakers).iteritems():\n",
    "        stats[speaker]['count'] = len(sents)\n",
    "        lengths = [len(s) for s in sents]\n",
    "        mean_length = np.mean(lengths)\n",
    "        std = np.std(lengths)\n",
    "        stats[speaker]['length'] = '%0.2f (+/- %0.2f)' % (mean_length, std*1.960)\n",
    "    \n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_stats = sentence_stats(dcr, sent_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUMP                2647       57.55 (+/- 91.10)   \n",
      "SANDERS              1948       90.94 (+/- 144.33)  \n",
      "CLINTON              1867       102.53 (+/- 147.05) \n",
      "RUBIO                1833       87.63 (+/- 119.96)  \n",
      "CRUZ                 1504       90.61 (+/- 131.77)  \n",
      "KASICH               1410       80.09 (+/- 124.65)  \n",
      "BUSH                 1374       78.07 (+/- 115.14)  \n",
      "CHRISTIE             961        86.49 (+/- 126.81)  \n",
      "CARSON               907        88.96 (+/- 122.32)  \n",
      "PAUL                 704        76.77 (+/- 100.19)  \n",
      "FIORINA              546        80.49 (+/- 120.56)  \n",
      "MUIR                 510        71.71 (+/- 124.24)  \n",
      "DICKERSON            485        73.13 (+/- 116.95)  \n",
      "BLITZER              456        50.85 (+/- 92.77)   \n",
      "TAPPER               429        56.00 (+/- 95.77)   \n",
      "COOPER               427        68.51 (+/- 114.06)  \n",
      "RADDATZ              321        78.61 (+/- 118.99)  \n",
      "HUCKABEE             270        87.93 (+/- 128.88)  \n",
      "KELLY                246        70.84 (+/- 120.65)  \n",
      "BAIER                246        69.13 (+/- 108.46)  \n",
      "HOLT                 244        77.03 (+/- 121.93)  \n",
      "BARTIROMO            228        62.58 (+/- 92.57)   \n",
      "CAVUTO               221        73.48 (+/- 126.41)  \n",
      "WALLACE              218        87.00 (+/- 128.28)  \n",
      "WALKER               205        89.07 (+/- 131.77)  \n",
      "BASH                 184        64.39 (+/- 102.15)  \n",
      "HEWITT               168        60.92 (+/- 98.34)   \n",
      "WEBB                 166        95.67 (+/- 156.82)  \n",
      "TODD                 156        72.14 (+/- 126.72)  \n",
      "QUINTANILLA          140        51.94 (+/- 101.41)  \n",
      "CHAFEE               139        66.40 (+/- 116.64)  \n",
      "QUICK                138        56.41 (+/- 93.35)   \n",
      "HARWOOD              122        64.58 (+/- 125.55)  \n",
      "MADDOW               108        96.61 (+/- 138.08)  \n",
      "BAKER                105        67.37 (+/- 129.39)  \n",
      "WOODRUFF             79         75.33 (+/- 136.83)  \n",
      "MITCHELL             79         65.62 (+/- 121.36)  \n",
      "IFILL                75         76.83 (+/- 104.97)  \n",
      "GARRETT              51         75.63 (+/- 132.59)  \n",
      "CORDES               43         60.63 (+/- 85.59)   \n",
      "COONEY               43         83.33 (+/- 122.54)  \n",
      "HAM                  42         70.31 (+/- 110.52)  \n",
      "MCELVEEN             40         73.67 (+/- 98.73)   \n",
      "STRASSEL             38         74.87 (+/- 115.17)  \n",
      "EPPERSON             31         78.74 (+/- 80.59)   \n",
      "QUESTION             29         95.62 (+/- 99.37)   \n",
      "LEVESQUE             29         85.79 (+/- 76.12)   \n",
      "LEMON                28         67.25 (+/- 119.12)  \n",
      "LOPEZ                24         85.75 (+/- 120.57)  \n",
      "UNKNOWN              23         34.70 (+/- 62.32)   \n",
      "OBRADOVICH           18         82.83 (+/- 100.75)  \n",
      "SANTELLI             13         55.00 (+/- 77.79)   \n",
      "CRAMER               11         74.91 (+/- 92.64)   \n",
      "ANNOUNCER            9          99.22 (+/- 123.31)  \n",
      "UNIDENTIFIED MALE    8          45.62 (+/- 63.01)   \n",
      "BROWNLEE             5          89.80 (+/- 75.46)   \n",
      "FRANCHESCA RAMSEY    4          93.50 (+/- 78.17)   \n",
      "FRANTA               4          77.50 (+/- 12.89)   \n",
      "PERRY                3          117.33 (+/- 73.71)  \n",
      "UNIDENTIFIED FEMALE  3          117.00 (+/- 26.34)  \n",
      "???                  3          43.67 (+/- 34.41)   \n",
      "MODERATOR            2          69.00 (+/- 115.64)  \n",
      "WILKINS              2          48.00 (+/- 64.68)   \n",
      "AUDIENCE             2          4.50 (+/- 0.98)     \n",
      "UNIDENTIFIED         1          27.00 (+/- 0.00)    \n",
      "MALE                 1          18.00 (+/- 0.00)    \n"
     ]
    }
   ],
   "source": [
    "for tup in sorted(sent_stats.iteritems(), key=lambda tup: tup[1]['count'], reverse=True):\n",
    "#     print \"%s\\t\\t\\tcount: %s\\tlength: %s\" % (tup[0], tup[1]['count'], tup[1]['length'])\n",
    "    print \"{: <20} {: <10} {: <20}\".format(tup[0], tup[1]['count'], tup[1]['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import make_scorer, classification_report, f1_score\n",
    "import codecs\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'BUSH', u'CARSON', u'CHRISTIE', u'CLINTON', u'CRUZ', u'KASICH', u'RUBIO', u'SANDERS', u'TRUMP']\n"
     ]
    }
   ],
   "source": [
    "def docs():\n",
    "    with codecs.open(sents_path, mode='r', encoding='utf-8') as _f:\n",
    "        for line in _f:\n",
    "            yield line\n",
    "\n",
    "def labels():\n",
    "    with codecs.open(labels_path, mode='r', encoding='utf-8') as _f:\n",
    "        for line in _f:\n",
    "            yield line.strip()\n",
    "labels_list = list(labels())\n",
    "candidates = sorted(list(set(labels_list)))\n",
    "docs_list = list(docs())\n",
    "print candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_cleaner = TransformerABC(prefilter_substitutions=['punct', 'strip', 'lower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores(clf, X, y):\n",
    "    scores = cross_val_score(clf, X, y, cv=10, scoring='f1_samples')\n",
    "    print scores\n",
    "    print \"\\n\"\n",
    "    # Use 1.96 * std b/c 95% of the data should lie in that range, \n",
    "    # which means this represents a 95% confidence interval\n",
    "    print \"F1: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 1.960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_vect = CountVectorizer(preprocessor=simple_cleaner, tokenizer=lambda s: s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_pipe = make_pipeline(simple_vect, MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fancy_scorer(y, y_pred, **kwargs):\n",
    "    # Print classification report\n",
    "    print classification_report(y, y_pred, target_names=candidates)\n",
    "    return f1_score(y, y_pred, labels=candidates, average='weighted')    \n",
    "\n",
    "def f1_weighted_scorer(y, y_pred, **kwargs):\n",
    "    return f1_score(y, y_pred, labels=candidates, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.73      0.39      0.50        83\n",
      "     CARSON       0.44      0.12      0.19        65\n",
      "   CHRISTIE       0.65      0.27      0.39        62\n",
      "    CLINTON       0.48      0.71      0.57       136\n",
      "       CRUZ       0.65      0.50      0.56       101\n",
      "     KASICH       0.59      0.42      0.49        84\n",
      "      RUBIO       0.47      0.61      0.53       122\n",
      "    SANDERS       0.56      0.65      0.60       123\n",
      "      TRUMP       0.54      0.76      0.63       113\n",
      "\n",
      "avg / total       0.56      0.54      0.52       889\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.63      0.35      0.45        83\n",
      "     CARSON       0.50      0.12      0.20        65\n",
      "   CHRISTIE       0.65      0.24      0.35        62\n",
      "    CLINTON       0.48      0.73      0.58       135\n",
      "       CRUZ       0.59      0.53      0.56       101\n",
      "     KASICH       0.53      0.48      0.50        84\n",
      "      RUBIO       0.47      0.62      0.54       122\n",
      "    SANDERS       0.61      0.68      0.65       123\n",
      "      TRUMP       0.53      0.63      0.58       113\n",
      "\n",
      "avg / total       0.55      0.54      0.52       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.67      0.35      0.46        83\n",
      "     CARSON       0.73      0.17      0.28        65\n",
      "   CHRISTIE       0.70      0.23      0.34        62\n",
      "    CLINTON       0.47      0.79      0.59       135\n",
      "       CRUZ       0.58      0.51      0.54       101\n",
      "     KASICH       0.56      0.45      0.50        84\n",
      "      RUBIO       0.52      0.63      0.57       122\n",
      "    SANDERS       0.59      0.60      0.59       123\n",
      "      TRUMP       0.51      0.67      0.58       113\n",
      "\n",
      "avg / total       0.57      0.54      0.52       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.71      0.42      0.53        83\n",
      "     CARSON       0.69      0.14      0.23        65\n",
      "   CHRISTIE       0.60      0.24      0.34        62\n",
      "    CLINTON       0.45      0.68      0.54       135\n",
      "       CRUZ       0.62      0.50      0.55       101\n",
      "     KASICH       0.55      0.43      0.48        84\n",
      "      RUBIO       0.49      0.68      0.57       122\n",
      "    SANDERS       0.60      0.66      0.63       123\n",
      "      TRUMP       0.57      0.73      0.64       113\n",
      "\n",
      "avg / total       0.57      0.54      0.53       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.72      0.35      0.47        83\n",
      "     CARSON       0.67      0.16      0.25        64\n",
      "   CHRISTIE       0.56      0.29      0.38        62\n",
      "    CLINTON       0.46      0.70      0.55       135\n",
      "       CRUZ       0.73      0.64      0.68       101\n",
      "     KASICH       0.67      0.36      0.47        83\n",
      "      RUBIO       0.48      0.62      0.54       121\n",
      "    SANDERS       0.57      0.70      0.63       123\n",
      "      TRUMP       0.54      0.72      0.62       113\n",
      "\n",
      "avg / total       0.58      0.55      0.54       885\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.74      0.35      0.48        83\n",
      "     CARSON       0.77      0.16      0.26        64\n",
      "   CHRISTIE       0.74      0.27      0.40        62\n",
      "    CLINTON       0.48      0.72      0.57       135\n",
      "       CRUZ       0.64      0.57      0.60       101\n",
      "     KASICH       0.65      0.51      0.57        83\n",
      "      RUBIO       0.46      0.60      0.53       121\n",
      "    SANDERS       0.59      0.64      0.61       123\n",
      "      TRUMP       0.55      0.77      0.64       113\n",
      "\n",
      "avg / total       0.60      0.56      0.54       885\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.65      0.34      0.44        83\n",
      "     CARSON       0.65      0.17      0.27        64\n",
      "   CHRISTIE       0.50      0.16      0.25        61\n",
      "    CLINTON       0.43      0.70      0.53       135\n",
      "       CRUZ       0.68      0.50      0.58       101\n",
      "     KASICH       0.61      0.36      0.45        83\n",
      "      RUBIO       0.45      0.69      0.54       121\n",
      "    SANDERS       0.63      0.69      0.66       122\n",
      "      TRUMP       0.56      0.70      0.62       113\n",
      "\n",
      "avg / total       0.56      0.53      0.51       883\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.61      0.30      0.40        83\n",
      "     CARSON       0.47      0.14      0.22        64\n",
      "   CHRISTIE       0.75      0.30      0.42        61\n",
      "    CLINTON       0.49      0.72      0.58       135\n",
      "       CRUZ       0.63      0.55      0.59       101\n",
      "     KASICH       0.51      0.39      0.44        83\n",
      "      RUBIO       0.54      0.65      0.59       121\n",
      "    SANDERS       0.57      0.66      0.61       122\n",
      "      TRUMP       0.51      0.73      0.60       113\n",
      "\n",
      "avg / total       0.56      0.54      0.52       883\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.65      0.41      0.51        82\n",
      "     CARSON       0.50      0.11      0.18        64\n",
      "   CHRISTIE       0.70      0.26      0.38        61\n",
      "    CLINTON       0.46      0.70      0.56       135\n",
      "       CRUZ       0.69      0.49      0.57       101\n",
      "     KASICH       0.43      0.28      0.34        83\n",
      "      RUBIO       0.46      0.67      0.54       121\n",
      "    SANDERS       0.59      0.62      0.61       122\n",
      "      TRUMP       0.54      0.75      0.63       113\n",
      "\n",
      "avg / total       0.55      0.53      0.51       882\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.81      0.41      0.55        82\n",
      "     CARSON       0.69      0.17      0.28        64\n",
      "   CHRISTIE       0.70      0.31      0.43        61\n",
      "    CLINTON       0.50      0.77      0.61       135\n",
      "       CRUZ       0.70      0.58      0.63       100\n",
      "     KASICH       0.60      0.47      0.53        83\n",
      "      RUBIO       0.46      0.62      0.53       121\n",
      "    SANDERS       0.63      0.65      0.64       122\n",
      "      TRUMP       0.55      0.74      0.63       112\n",
      "\n",
      "avg / total       0.61      0.57      0.56       880\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.52231296,  0.51771904,  0.51920321,  0.5269497 ,  0.53510422,\n",
       "        0.53976703,  0.51430653,  0.52304192,  0.50787193,  0.5573232 ])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(simple_pipe, docs_list, labels_list, cv=10, scoring=make_scorer(fancy_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_cleaner = TransformerABC(prefilter_substitutions=['strip', 'lower'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_tokenizer(s):\n",
    "    return s.split()\n",
    "\n",
    "def lemmas_tokenizer(s):\n",
    "    return [s.lemma_ for s in nlp(s)]\n",
    "\n",
    "def lemmas_no_punct_tokenizer(s):\n",
    "    return [s.lemma_ for s in nlp(s) if not any([s.is_punct, s.is_space])]\n",
    "\n",
    "def lemmas_no_punt_no_num(s):\n",
    "    return [s.lemma_ for s in nlp(s) if not any([s.is_punct, s.is_space, s.is_digit, s.is_like_num])]\n",
    "\n",
    "def lemmas_sub_nums(s):\n",
    "    pass\n",
    "\n",
    "def lemmas_no_punct_sub_nums():\n",
    "    pass\n",
    "\n",
    "# Spacy tokenization, no lemmatization..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sk_stops = ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(preprocessor=simple_cleaner)),\n",
    "        ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "sgd_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(preprocessor=simple_cleaner)),\n",
    "        ('clf', SGDClassifier())\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(preprocessor=simple_cleaner)),\n",
    "        ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "svm_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(preprocessor=simple_cleaner)),\n",
    "        ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "# TODO:\n",
    "# - TFIDF\n",
    "# - log-count ratio?\n",
    "# - binarize\n",
    "\n",
    "pipes = [mnb_pipe, sgd_pipe, rf_pipe, svm_pipe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_grid_params = {\n",
    "    'vect__ngram_range': ((1,1),(1,2),(1,3)),\n",
    "    'vect__stop_words': ('english', None),\n",
    "    'vect__tokenizer': (simple_tokenizer, lemmas_tokenizer, lemmas_no_punct_tokenizer),\n",
    "    'clf__alpha': (0.001, 0.01, 0.1, 1)\n",
    "}\n",
    "\n",
    "sgd_grid_params = {\n",
    "    'vect__ngram_range': ((1,1),(1,2),(1,3)),\n",
    "    'vect__stop_words': ('english', None),\n",
    "    'vect__tokenizer': (lemmas_tokenizer, lemmas_no_punct_tokenizer),\n",
    "    'clf__loss': ('hinge', 'log', 'perceptron'),\n",
    "    'clf__penalty': ('l1', 'l2', 'elasticnet'),\n",
    "    'clf__alpha': (0.0001, 0.00001, 0.000001),\n",
    "    'clf__l1_ratio': (0.15, 0.05, 0.005),\n",
    "    'clf__fit_intercept': (True, False),\n",
    "    'clf__n_iter': (10, 50, 85)\n",
    "}\n",
    "\n",
    "rf_grid_params = {}\n",
    "\n",
    "svm_grid_params = {}\n",
    "\n",
    "grids = [mnb_grid_params, sgd_grid_params, rf_grid_params, svm_grid_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "fmt = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hndlr = logging.StreamHandler()\n",
    "hndlr.setFormatter(fmt)\n",
    "logger.addHandler(hndlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_grid(pipe, params, data, labels):\n",
    "    grid_search = GridSearchCV(pipe, params, n_jobs=-1, scoring=make_scorer(f1_weighted_scorer), cv=3, verbose=1)\n",
    "    \n",
    "    print \"Performing grid search...\"\n",
    "    print \"pipeline:\", [name for name, _ in pipe.steps]\n",
    "    print \"parameters:\"\n",
    "    pprint(params)\n",
    "    \n",
    "    t0 = time()\n",
    "    grid_search.fit(data, labels)\n",
    "    \n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "    print ''\n",
    "    \n",
    "    print \"Best score: %0.3f\" % grid_search.best_score_\n",
    "    print \"Best parameters set:\"\n",
    "    \n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()):\n",
    "        print \"\\t%s: %r\" % (param_name, best_parameters[param_name])\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.001, 0.01, 0.1, 1),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
      " 'vect__stop_words': ('english', None),\n",
      " 'vect__tokenizer': (<function simple_tokenizer at 0x7f5254a35b90>,\n",
      "                     <function lemmas_tokenizer at 0x7f5254a35f50>,\n",
      "                     <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>)}\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   54.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 343.143s\n",
      "\n",
      "Best score: 0.634\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__stop_words: None\n",
      "\tvect__tokenizer: <function lemmas_tokenizer at 0x7f5254a35f50>\n"
     ]
    }
   ],
   "source": [
    "mnb_grid = process_grid(mnb_pipe, mnb_grid_params, docs_list, labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.48942, std: 0.00521, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.50824, std: 0.00670, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.50508, std: 0.00504, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.50453, std: 0.00474, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.52849, std: 0.00333, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.52745, std: 0.00589, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.53247, std: 0.00492, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.54907, std: 0.00754, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.55504, std: 0.00831, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.58753, std: 0.00237, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.60793, std: 0.00317, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.60296, std: 0.00562, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.53292, std: 0.00350, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.54406, std: 0.00859, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.55488, std: 0.00663, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.59263, std: 0.00308, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.62247, std: 0.00773, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.61508, std: 0.00443, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.49806, std: 0.00343, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.51535, std: 0.00709, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.51180, std: 0.00525, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.51934, std: 0.00334, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.54093, std: 0.00389, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.53907, std: 0.00446, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.53851, std: 0.00585, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.55860, std: 0.00945, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.56025, std: 0.00923, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.59495, std: 0.00381, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.61818, std: 0.00467, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.61331, std: 0.00467, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.53928, std: 0.00623, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.55093, std: 0.00974, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.56115, std: 0.00577, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.60027, std: 0.00290, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.63066, std: 0.00851, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.62257, std: 0.00493, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.51106, std: 0.00105, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.52732, std: 0.00712, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.52160, std: 0.00438, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.53478, std: 0.00584, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.55604, std: 0.00495, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.55639, std: 0.00625, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.54672, std: 0.00760, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.56291, std: 0.00959, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.56512, std: 0.00947, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.59977, std: 0.00027, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.62333, std: 0.00555, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.62161, std: 0.00417, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.54829, std: 0.00826, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.55324, std: 0.00495, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.56679, std: 0.00731, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.59867, std: 0.00144, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.63438, std: 0.00746, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.62853, std: 0.00621, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.47899, std: 0.00304, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.48907, std: 0.00569, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.49457, std: 0.00437, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.45332, std: 0.00564, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.49761, std: 0.00234, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.50009, std: 0.00206, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.48717, std: 0.01019, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.48016, std: 0.00682, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.50698, std: 0.00957, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.46347, std: 0.00666, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.49825, std: 0.00197, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.49807, std: 0.00569, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.48898, std: 0.00843, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.47929, std: 0.00119, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.50588, std: 0.00801, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.46889, std: 0.00574, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.50065, std: 0.00609, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.50465, std: 0.00494, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 1}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_best.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       1.00      1.00      1.00       828\n",
      "     CARSON       1.00      1.00      1.00       644\n",
      "   CHRISTIE       1.00      1.00      1.00       616\n",
      "    CLINTON       1.00      1.00      1.00      1351\n",
      "       CRUZ       1.00      1.00      1.00      1009\n",
      "     KASICH       1.00      1.00      1.00       834\n",
      "      RUBIO       1.00      1.00      1.00      1214\n",
      "    SANDERS       1.00      1.00      1.00      1226\n",
      "      TRUMP       1.00      1.00      1.00      1129\n",
      "\n",
      "avg / total       1.00      1.00      1.00      8851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(labels_list, mnb_best.best_estimator_.predict(docs_list), target_names=candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_tuned_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(preprocessor=simple_cleaner,\n",
    "                                 ngram_range=(1,3),\n",
    "                                 tokenizer=lemmas_tokenizer)),\n",
    "        ('clf', MultinomialNB(alpha=0.01))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.64      0.63      0.63        83\n",
      "     CARSON       0.60      0.55      0.58        65\n",
      "   CHRISTIE       0.50      0.47      0.48        62\n",
      "    CLINTON       0.69      0.79      0.73       136\n",
      "       CRUZ       0.73      0.61      0.67       101\n",
      "     KASICH       0.58      0.62      0.60        84\n",
      "      RUBIO       0.66      0.62      0.64       122\n",
      "    SANDERS       0.77      0.74      0.76       123\n",
      "      TRUMP       0.67      0.75      0.71       113\n",
      "\n",
      "avg / total       0.66      0.66      0.66       889\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.65      0.60      0.63        83\n",
      "     CARSON       0.55      0.51      0.53        65\n",
      "   CHRISTIE       0.59      0.60      0.59        62\n",
      "    CLINTON       0.64      0.73      0.68       135\n",
      "       CRUZ       0.73      0.68      0.70       101\n",
      "     KASICH       0.56      0.63      0.59        84\n",
      "      RUBIO       0.64      0.63      0.63       122\n",
      "    SANDERS       0.79      0.65      0.71       123\n",
      "      TRUMP       0.61      0.66      0.64       113\n",
      "\n",
      "avg / total       0.65      0.64      0.64       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.59      0.57      0.58        83\n",
      "     CARSON       0.51      0.54      0.52        65\n",
      "   CHRISTIE       0.45      0.48      0.47        62\n",
      "    CLINTON       0.68      0.76      0.72       135\n",
      "       CRUZ       0.77      0.62      0.69       101\n",
      "     KASICH       0.54      0.58      0.56        84\n",
      "      RUBIO       0.67      0.65      0.66       122\n",
      "    SANDERS       0.73      0.65      0.69       123\n",
      "      TRUMP       0.65      0.71      0.68       113\n",
      "\n",
      "avg / total       0.64      0.64      0.64       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.70      0.60      0.65        83\n",
      "     CARSON       0.68      0.46      0.55        65\n",
      "   CHRISTIE       0.48      0.56      0.52        62\n",
      "    CLINTON       0.62      0.62      0.62       135\n",
      "       CRUZ       0.76      0.68      0.72       101\n",
      "     KASICH       0.54      0.58      0.56        84\n",
      "      RUBIO       0.66      0.72      0.69       122\n",
      "    SANDERS       0.67      0.67      0.67       123\n",
      "      TRUMP       0.71      0.79      0.74       113\n",
      "\n",
      "avg / total       0.65      0.65      0.65       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.70      0.61      0.65        83\n",
      "     CARSON       0.61      0.69      0.65        64\n",
      "   CHRISTIE       0.68      0.58      0.63        62\n",
      "    CLINTON       0.68      0.70      0.69       135\n",
      "       CRUZ       0.75      0.71      0.73       101\n",
      "     KASICH       0.61      0.59      0.60        83\n",
      "      RUBIO       0.64      0.62      0.63       121\n",
      "    SANDERS       0.75      0.79      0.77       123\n",
      "      TRUMP       0.74      0.81      0.77       113\n",
      "\n",
      "avg / total       0.69      0.69      0.69       885\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.72      0.57      0.64        83\n",
      "     CARSON       0.47      0.38      0.42        64\n",
      "   CHRISTIE       0.58      0.55      0.56        62\n",
      "    CLINTON       0.66      0.68      0.67       135\n",
      "       CRUZ       0.77      0.71      0.74       101\n",
      "     KASICH       0.62      0.58      0.60        83\n",
      "      RUBIO       0.63      0.64      0.63       121\n",
      "    SANDERS       0.72      0.78      0.75       123\n",
      "      TRUMP       0.65      0.82      0.72       113\n",
      "\n",
      "avg / total       0.66      0.66      0.66       885\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.64      0.57      0.60        83\n",
      "     CARSON       0.50      0.56      0.53        64\n",
      "   CHRISTIE       0.47      0.43      0.45        61\n",
      "    CLINTON       0.67      0.74      0.70       135\n",
      "       CRUZ       0.73      0.73      0.73       101\n",
      "     KASICH       0.64      0.57      0.60        83\n",
      "      RUBIO       0.69      0.69      0.69       121\n",
      "    SANDERS       0.75      0.70      0.73       122\n",
      "      TRUMP       0.67      0.73      0.70       113\n",
      "\n",
      "avg / total       0.66      0.66      0.66       883\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.71      0.57      0.63        83\n",
      "     CARSON       0.54      0.50      0.52        64\n",
      "   CHRISTIE       0.62      0.56      0.59        61\n",
      "    CLINTON       0.65      0.64      0.65       135\n",
      "       CRUZ       0.75      0.65      0.70       101\n",
      "     KASICH       0.64      0.71      0.67        83\n",
      "      RUBIO       0.70      0.61      0.65       121\n",
      "    SANDERS       0.71      0.74      0.72       122\n",
      "      TRUMP       0.63      0.87      0.73       113\n",
      "\n",
      "avg / total       0.67      0.66      0.66       883\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.71      0.59      0.64        82\n",
      "     CARSON       0.56      0.53      0.54        64\n",
      "   CHRISTIE       0.61      0.59      0.60        61\n",
      "    CLINTON       0.61      0.66      0.64       135\n",
      "       CRUZ       0.80      0.73      0.76       101\n",
      "     KASICH       0.53      0.53      0.53        83\n",
      "      RUBIO       0.70      0.72      0.71       121\n",
      "    SANDERS       0.78      0.73      0.75       122\n",
      "      TRUMP       0.66      0.79      0.72       113\n",
      "\n",
      "avg / total       0.67      0.67      0.67       882\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.70      0.59      0.64        82\n",
      "     CARSON       0.49      0.44      0.46        64\n",
      "   CHRISTIE       0.60      0.59      0.60        61\n",
      "    CLINTON       0.66      0.72      0.69       135\n",
      "       CRUZ       0.79      0.68      0.73       100\n",
      "     KASICH       0.60      0.61      0.61        83\n",
      "      RUBIO       0.62      0.66      0.64       121\n",
      "    SANDERS       0.80      0.78      0.79       122\n",
      "      TRUMP       0.68      0.77      0.72       112\n",
      "\n",
      "avg / total       0.67      0.67      0.67       880\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.66223059,  0.64484832,  0.63723762,  0.64807637,  0.68796454,\n",
       "        0.65502752,  0.65799286,  0.66213803,  0.66871257,  0.66857936])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mnb_tuned_pipe, docs_list, labels_list, cv=10, scoring=make_scorer(fancy_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6592807780000001"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ 0.66223059,  0.64484832,  0.63723762,  0.64807637,  0.68796454,\n",
    "        0.65502752,  0.65799286,  0.66213803,  0.66871257,  0.66857936])/10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.0001, 1e-05, 1e-06),\n",
      " 'clf__fit_intercept': (True, False),\n",
      " 'clf__l1_ratio': (0.15, 0.05, 0.005),\n",
      " 'clf__loss': ('hinge', 'log', 'perceptron'),\n",
      " 'clf__n_iter': (10, 50, 85),\n",
      " 'clf__penalty': ('l1', 'l2', 'elasticnet'),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
      " 'vect__stop_words': ('english', None),\n",
      " 'vect__tokenizer': (<function lemmas_tokenizer at 0x7f52783ee848>,\n",
      "                     <function lemmas_no_punct_tokenizer at 0x7f5254e21b18>)}\n",
      "Fitting 3 folds for each of 5832 candidates, totalling 17496 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 30.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 48.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 69.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 94.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 123.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 155.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 192.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 232.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 276.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 324.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 376.2min\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed: 431.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed: 491.3min\n",
      "[Parallel(n_jobs=-1)]: Done 14434 tasks      | elapsed: 554.8min\n",
      "[Parallel(n_jobs=-1)]: Done 16184 tasks      | elapsed: 622.4min\n",
      "[Parallel(n_jobs=-1)]: Done 17496 out of 17496 | elapsed: 672.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 40381.515s\n",
      "\n",
      "Best score: 0.579\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.0001\n",
      "\tclf__fit_intercept: False\n",
      "\tclf__l1_ratio: 0.05\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__n_iter: 85\n",
      "\tclf__penalty: 'l2'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__stop_words: None\n",
      "\tvect__tokenizer: <function lemmas_no_punct_tokenizer at 0x7f5254e21b18>\n"
     ]
    }
   ],
   "source": [
    "sgd_grid = process_grid(sgd_pipe, sgd_grid_params, docs_list, labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.001, 0.01, 0.1, 1),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
      " 'vect__stop_words': ('english', None),\n",
      " 'vect__tokenizer': (<function simple_tokenizer at 0x7f5254a35b90>,\n",
      "                     <function lemmas_tokenizer at 0x7f5254a35f50>,\n",
      "                     <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>)}\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:  5.0min\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed: 23.6min\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed: 54.2min\n",
      "[Parallel(n_jobs=1)]: Done 720 out of 720 | elapsed: 89.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5363.179s\n",
      "\n",
      "Best score: 0.659\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__stop_words: None\n",
      "\tvect__tokenizer: <function lemmas_tokenizer at 0x7f5254a35f50>\n"
     ]
    }
   ],
   "source": [
    "process_grid(mnb_pipe, mnb_grid_params, docs_list, labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt = u\"Listening to this, do you think this is the tone — this immigration debate that republicans need to take to win back Hispanics into our party especially states like where we are in Nevada that has a pretty Hispanic community?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'listen | to | this | , | do | you | think | this | be | the | tone | -- | this | immigration | debate | that | republicans | need | to | take | to | win | back | hispanic | into | our | party | especially | state | like | where | we | be | in | nevada | that | have | a | pretty | hispanic | community | ?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = nlp(txt)\n",
    "' | '.join(tok.lemma_ for tok in toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'--' in set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks[11].is_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
