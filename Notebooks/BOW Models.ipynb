{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from spacy.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/Storage/Coding_Projects/Candidate_Classifier'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/Storage/Coding_Projects/Candidate_Classifier'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from candidate_classifier.nltk_model import NgramModel\n",
    "from candidate_classifier import utils\n",
    "from nltk.probability import LaplaceProbDist, LidstoneProbDist\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os\n",
    "from candidate_classifier.debate_corpus_reader import DebateCorpusReader\n",
    "from candidate_classifier.string_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = English(entity=False, load_vectors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransformerWrapper(object):\n",
    "    def __init__(self, transformer):\n",
    "        self.transformer = transformer\n",
    "    \n",
    "    def tokenize(self, s):\n",
    "        return self.transformer(s)\n",
    "\n",
    "class DummyTokenizer(object):\n",
    "    def tokenize(self, s):\n",
    "        return s\n",
    "    \n",
    "def sent_tokenizer(s):\n",
    "    doc = nlp(s)\n",
    "    return [u''.join(t.text_with_ws for t in sent) for sent in doc.sents]\n",
    "\n",
    "\n",
    "# def word_tokenizer(s):\n",
    "#     toks = nlp(s)\n",
    "#     return ['<S>'] + [t.lower_ for t in toks] + ['</S>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace [*] with ''\n",
    "# Replace '. . .' with '...'\n",
    "# Replace multiple ellipses with single ...\n",
    "# Remove all sentences that end with ...\n",
    "\n",
    "BRACKET_PATTERN = re.compile(r\"\\[[a-zA-Z ]*\\]\", re.U)\n",
    "SPACED_ELLIPSIS_PATTERN = re.compile(r\"((?:\\.\\s){3})\")\n",
    "MULTI_ELLIPSIS_PATTERN = re.compile(r\"(?:(?:\\.){3} ?)+\")\n",
    "ELLIPSIS_BRACKET_PATTERN = re.compile(r\"(?:(?:\\.){3}) *\\[[a-zA-Z ]*\\] *(?:(?:\\.){3} ?)\")\n",
    "ENDS_WITH_ELLIPSIS = lambda s: s[-3:] == '...'\n",
    "STARTS_WITH_ELLIPSIS = lambda s: s[:3] == '...'\n",
    "STARTS_WITH_DASH = lambda s: s[0] == '-'\n",
    "ENDS_WITH_DASH = lambda s: s[-1] == '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_transformer = TransformerABC(\n",
    "    prefilter_substitutions=['html entities',\n",
    "                             'deaccent',\n",
    "                             'whitespace',\n",
    "                             (ELLIPSIS_BRACKET_PATTERN, ' '),\n",
    "                             BRACKET_PATTERN,\n",
    "                             (SPACED_ELLIPSIS_PATTERN, '...'),\n",
    "                             (MULTI_ELLIPSIS_PATTERN, '...'),\n",
    "                             'whitespace',\n",
    "                             'strip'],\n",
    "    tokenizer=sent_tokenizer)\n",
    "\n",
    "sent_filter = TransformerABC(\n",
    "    prefilter_substitutions=['puntuation', 'strip'],\n",
    "    filters=[('len', 49)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcr = DebateCorpusReader('candidate_classifier/data/raw', '.*\\.txt', \n",
    "                             sent_tokenizer=TransformerWrapper(doc_transformer), \n",
    "                             word_tokenizer=DummyTokenizer())\n",
    "candidates = ['BUSH', 'CARSON', 'CHRISTIE', 'CRUZ', 'KASICH', 'RUBIO', 'TRUMP', 'CLINTON', 'SANDERS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess to sentences\n",
    "I have officially decided that the task is classifying sentences and snippits.  As such, each document will be a sentence.  \n",
    "\n",
    "#### Precrocessing Steps\n",
    "- Tokenize to sentences and normalize encoding\n",
    "- remove all non-ascii characters\n",
    "- All whitespace to spaces (no newlines)\n",
    "- Filter all sentences that are less than 50 characters (after removing all punctuation)\n",
    "- Group all sentences together with their labels and shuffle them\n",
    "- Write text and labels to (separate) line-delimited files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_sents = []\n",
    "for label, sents in dcr.grouped_sents(speakers=candidates).iteritems():\n",
    "    for sent in sents:\n",
    "#             stripped = sent.strip(string.punctuation+ ' \\n\\t')\n",
    "#             if len(stripped) >= 35:\n",
    "#                 cleaned_sents.append((label, sent))\n",
    "        if sent_filter(sent):\n",
    "            cleaned_sents.append((label, sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8851"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(cleaned_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_path = 'candidate_classifier/data/processed/clean_sents.txt'\n",
    "labels_path = 'candidate_classifier/data/processed/sent_labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "sents_file = codecs.open(sents_path, mode='w', encoding='utf-8')\n",
    "labels_file = codecs.open(labels_path, mode='w', encoding='utf-8')\n",
    "    \n",
    "for sent in cleaned_sents:\n",
    "    sents_file.write(u'%s\\n' % sent[1])\n",
    "    labels_file.write(u'%s\\n' % sent[0])\n",
    "\n",
    "sents_file.close()\n",
    "labels_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_stats(dcr, filt, speakers=None):\n",
    "    \"\"\"\n",
    "    Returns a dictionary for the given speakers with stats for all sentences\n",
    "    dcr is a DebateCorpusReader\n",
    "    filt is a callable that will turn sentences we want to filter into a falsy value\n",
    "    \"\"\"\n",
    "    if speakers is None:\n",
    "        speakers = dcr.speakers()\n",
    "    stats = {s:dict() for s in speakers}\n",
    "    \n",
    "    for speaker, sents in dcr.grouped_sents(speakers=speakers).iteritems():\n",
    "        stats[speaker]['count'] = len(sents)\n",
    "        lengths = [len(s) for s in sents]\n",
    "        mean_length = np.mean(lengths)\n",
    "        std = np.std(lengths)\n",
    "        stats[speaker]['length'] = '%0.2f (+/- %0.2f)' % (mean_length, std*1.960)\n",
    "    \n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_stats = sentence_stats(dcr, sent_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUMP                2647       57.55 (+/- 91.10)   \n",
      "SANDERS              1948       90.94 (+/- 144.33)  \n",
      "CLINTON              1867       102.53 (+/- 147.05) \n",
      "RUBIO                1833       87.63 (+/- 119.96)  \n",
      "CRUZ                 1504       90.61 (+/- 131.77)  \n",
      "KASICH               1410       80.09 (+/- 124.65)  \n",
      "BUSH                 1374       78.07 (+/- 115.14)  \n",
      "CHRISTIE             961        86.49 (+/- 126.81)  \n",
      "CARSON               907        88.96 (+/- 122.32)  \n",
      "PAUL                 704        76.77 (+/- 100.19)  \n",
      "FIORINA              546        80.49 (+/- 120.56)  \n",
      "MUIR                 510        71.71 (+/- 124.24)  \n",
      "DICKERSON            485        73.13 (+/- 116.95)  \n",
      "BLITZER              456        50.85 (+/- 92.77)   \n",
      "TAPPER               429        56.00 (+/- 95.77)   \n",
      "COOPER               427        68.51 (+/- 114.06)  \n",
      "RADDATZ              321        78.61 (+/- 118.99)  \n",
      "HUCKABEE             270        87.93 (+/- 128.88)  \n",
      "KELLY                246        70.84 (+/- 120.65)  \n",
      "BAIER                246        69.13 (+/- 108.46)  \n",
      "HOLT                 244        77.03 (+/- 121.93)  \n",
      "BARTIROMO            228        62.58 (+/- 92.57)   \n",
      "CAVUTO               221        73.48 (+/- 126.41)  \n",
      "WALLACE              218        87.00 (+/- 128.28)  \n",
      "WALKER               205        89.07 (+/- 131.77)  \n",
      "BASH                 184        64.39 (+/- 102.15)  \n",
      "HEWITT               168        60.92 (+/- 98.34)   \n",
      "WEBB                 166        95.67 (+/- 156.82)  \n",
      "TODD                 156        72.14 (+/- 126.72)  \n",
      "QUINTANILLA          140        51.94 (+/- 101.41)  \n",
      "CHAFEE               139        66.40 (+/- 116.64)  \n",
      "QUICK                138        56.41 (+/- 93.35)   \n",
      "HARWOOD              122        64.58 (+/- 125.55)  \n",
      "MADDOW               108        96.61 (+/- 138.08)  \n",
      "BAKER                105        67.37 (+/- 129.39)  \n",
      "WOODRUFF             79         75.33 (+/- 136.83)  \n",
      "MITCHELL             79         65.62 (+/- 121.36)  \n",
      "IFILL                75         76.83 (+/- 104.97)  \n",
      "GARRETT              51         75.63 (+/- 132.59)  \n",
      "CORDES               43         60.63 (+/- 85.59)   \n",
      "COONEY               43         83.33 (+/- 122.54)  \n",
      "HAM                  42         70.31 (+/- 110.52)  \n",
      "MCELVEEN             40         73.67 (+/- 98.73)   \n",
      "STRASSEL             38         74.87 (+/- 115.17)  \n",
      "EPPERSON             31         78.74 (+/- 80.59)   \n",
      "QUESTION             29         95.62 (+/- 99.37)   \n",
      "LEVESQUE             29         85.79 (+/- 76.12)   \n",
      "LEMON                28         67.25 (+/- 119.12)  \n",
      "LOPEZ                24         85.75 (+/- 120.57)  \n",
      "UNKNOWN              23         34.70 (+/- 62.32)   \n",
      "OBRADOVICH           18         82.83 (+/- 100.75)  \n",
      "SANTELLI             13         55.00 (+/- 77.79)   \n",
      "CRAMER               11         74.91 (+/- 92.64)   \n",
      "ANNOUNCER            9          99.22 (+/- 123.31)  \n",
      "UNIDENTIFIED MALE    8          45.62 (+/- 63.01)   \n",
      "BROWNLEE             5          89.80 (+/- 75.46)   \n",
      "FRANCHESCA RAMSEY    4          93.50 (+/- 78.17)   \n",
      "FRANTA               4          77.50 (+/- 12.89)   \n",
      "PERRY                3          117.33 (+/- 73.71)  \n",
      "UNIDENTIFIED FEMALE  3          117.00 (+/- 26.34)  \n",
      "???                  3          43.67 (+/- 34.41)   \n",
      "MODERATOR            2          69.00 (+/- 115.64)  \n",
      "WILKINS              2          48.00 (+/- 64.68)   \n",
      "AUDIENCE             2          4.50 (+/- 0.98)     \n",
      "UNIDENTIFIED         1          27.00 (+/- 0.00)    \n",
      "MALE                 1          18.00 (+/- 0.00)    \n"
     ]
    }
   ],
   "source": [
    "for tup in sorted(sent_stats.iteritems(), key=lambda tup: tup[1]['count'], reverse=True):\n",
    "#     print \"%s\\t\\t\\tcount: %s\\tlength: %s\" % (tup[0], tup[1]['count'], tup[1]['length'])\n",
    "    print \"{: <20} {: <10} {: <20}\".format(tup[0], tup[1]['count'], tup[1]['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import make_scorer, classification_report, f1_score\n",
    "import codecs\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'BUSH', u'CARSON', u'CHRISTIE', u'CLINTON', u'CRUZ', u'KASICH', u'RUBIO', u'SANDERS', u'TRUMP']\n"
     ]
    }
   ],
   "source": [
    "def docs():\n",
    "    with codecs.open(sents_path, mode='r', encoding='utf-8') as _f:\n",
    "        for line in _f:\n",
    "            yield line\n",
    "\n",
    "def labels():\n",
    "    with codecs.open(labels_path, mode='r', encoding='utf-8') as _f:\n",
    "        for line in _f:\n",
    "            yield line.strip()\n",
    "labels_list = list(labels())\n",
    "candidates = sorted(list(set(labels_list)))\n",
    "docs_list = list(docs())\n",
    "print candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_cleaner = TransformerABC(prefilter_substitutions=['punct', 'strip', 'lower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores(clf, X, y):\n",
    "    scores = cross_val_score(clf, X, y, cv=10, scoring='f1_samples')\n",
    "    print scores\n",
    "    print \"\\n\"\n",
    "    # Use 1.96 * std b/c 95% of the data should lie in that range, \n",
    "    # which means this represents a 95% confidence interval\n",
    "    print \"F1: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 1.960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_vect = CountVectorizer(preprocessor=simple_cleaner, tokenizer=lambda s: s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_pipe = make_pipeline(simple_vect, MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fancy_scorer(y, y_pred, **kwargs):\n",
    "    # Print classification report\n",
    "    print classification_report(y, y_pred, target_names=candidates)\n",
    "    return f1_score(y, y_pred, labels=candidates, average='weighted')    \n",
    "\n",
    "def f1_weighted_scorer(y, y_pred, **kwargs):\n",
    "    return f1_score(y, y_pred, labels=candidates, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.73      0.39      0.50        83\n",
      "     CARSON       0.44      0.12      0.19        65\n",
      "   CHRISTIE       0.65      0.27      0.39        62\n",
      "    CLINTON       0.48      0.71      0.57       136\n",
      "       CRUZ       0.65      0.50      0.56       101\n",
      "     KASICH       0.59      0.42      0.49        84\n",
      "      RUBIO       0.47      0.61      0.53       122\n",
      "    SANDERS       0.56      0.65      0.60       123\n",
      "      TRUMP       0.54      0.76      0.63       113\n",
      "\n",
      "avg / total       0.56      0.54      0.52       889\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.63      0.35      0.45        83\n",
      "     CARSON       0.50      0.12      0.20        65\n",
      "   CHRISTIE       0.65      0.24      0.35        62\n",
      "    CLINTON       0.48      0.73      0.58       135\n",
      "       CRUZ       0.59      0.53      0.56       101\n",
      "     KASICH       0.53      0.48      0.50        84\n",
      "      RUBIO       0.47      0.62      0.54       122\n",
      "    SANDERS       0.61      0.68      0.65       123\n",
      "      TRUMP       0.53      0.63      0.58       113\n",
      "\n",
      "avg / total       0.55      0.54      0.52       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.67      0.35      0.46        83\n",
      "     CARSON       0.73      0.17      0.28        65\n",
      "   CHRISTIE       0.70      0.23      0.34        62\n",
      "    CLINTON       0.47      0.79      0.59       135\n",
      "       CRUZ       0.58      0.51      0.54       101\n",
      "     KASICH       0.56      0.45      0.50        84\n",
      "      RUBIO       0.52      0.63      0.57       122\n",
      "    SANDERS       0.59      0.60      0.59       123\n",
      "      TRUMP       0.51      0.67      0.58       113\n",
      "\n",
      "avg / total       0.57      0.54      0.52       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.71      0.42      0.53        83\n",
      "     CARSON       0.69      0.14      0.23        65\n",
      "   CHRISTIE       0.60      0.24      0.34        62\n",
      "    CLINTON       0.45      0.68      0.54       135\n",
      "       CRUZ       0.62      0.50      0.55       101\n",
      "     KASICH       0.55      0.43      0.48        84\n",
      "      RUBIO       0.49      0.68      0.57       122\n",
      "    SANDERS       0.60      0.66      0.63       123\n",
      "      TRUMP       0.57      0.73      0.64       113\n",
      "\n",
      "avg / total       0.57      0.54      0.53       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.72      0.35      0.47        83\n",
      "     CARSON       0.67      0.16      0.25        64\n",
      "   CHRISTIE       0.56      0.29      0.38        62\n",
      "    CLINTON       0.46      0.70      0.55       135\n",
      "       CRUZ       0.73      0.64      0.68       101\n",
      "     KASICH       0.67      0.36      0.47        83\n",
      "      RUBIO       0.48      0.62      0.54       121\n",
      "    SANDERS       0.57      0.70      0.63       123\n",
      "      TRUMP       0.54      0.72      0.62       113\n",
      "\n",
      "avg / total       0.58      0.55      0.54       885\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.74      0.35      0.48        83\n",
      "     CARSON       0.77      0.16      0.26        64\n",
      "   CHRISTIE       0.74      0.27      0.40        62\n",
      "    CLINTON       0.48      0.72      0.57       135\n",
      "       CRUZ       0.64      0.57      0.60       101\n",
      "     KASICH       0.65      0.51      0.57        83\n",
      "      RUBIO       0.46      0.60      0.53       121\n",
      "    SANDERS       0.59      0.64      0.61       123\n",
      "      TRUMP       0.55      0.77      0.64       113\n",
      "\n",
      "avg / total       0.60      0.56      0.54       885\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.65      0.34      0.44        83\n",
      "     CARSON       0.65      0.17      0.27        64\n",
      "   CHRISTIE       0.50      0.16      0.25        61\n",
      "    CLINTON       0.43      0.70      0.53       135\n",
      "       CRUZ       0.68      0.50      0.58       101\n",
      "     KASICH       0.61      0.36      0.45        83\n",
      "      RUBIO       0.45      0.69      0.54       121\n",
      "    SANDERS       0.63      0.69      0.66       122\n",
      "      TRUMP       0.56      0.70      0.62       113\n",
      "\n",
      "avg / total       0.56      0.53      0.51       883\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.61      0.30      0.40        83\n",
      "     CARSON       0.47      0.14      0.22        64\n",
      "   CHRISTIE       0.75      0.30      0.42        61\n",
      "    CLINTON       0.49      0.72      0.58       135\n",
      "       CRUZ       0.63      0.55      0.59       101\n",
      "     KASICH       0.51      0.39      0.44        83\n",
      "      RUBIO       0.54      0.65      0.59       121\n",
      "    SANDERS       0.57      0.66      0.61       122\n",
      "      TRUMP       0.51      0.73      0.60       113\n",
      "\n",
      "avg / total       0.56      0.54      0.52       883\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.65      0.41      0.51        82\n",
      "     CARSON       0.50      0.11      0.18        64\n",
      "   CHRISTIE       0.70      0.26      0.38        61\n",
      "    CLINTON       0.46      0.70      0.56       135\n",
      "       CRUZ       0.69      0.49      0.57       101\n",
      "     KASICH       0.43      0.28      0.34        83\n",
      "      RUBIO       0.46      0.67      0.54       121\n",
      "    SANDERS       0.59      0.62      0.61       122\n",
      "      TRUMP       0.54      0.75      0.63       113\n",
      "\n",
      "avg / total       0.55      0.53      0.51       882\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.81      0.41      0.55        82\n",
      "     CARSON       0.69      0.17      0.28        64\n",
      "   CHRISTIE       0.70      0.31      0.43        61\n",
      "    CLINTON       0.50      0.77      0.61       135\n",
      "       CRUZ       0.70      0.58      0.63       100\n",
      "     KASICH       0.60      0.47      0.53        83\n",
      "      RUBIO       0.46      0.62      0.53       121\n",
      "    SANDERS       0.63      0.65      0.64       122\n",
      "      TRUMP       0.55      0.74      0.63       112\n",
      "\n",
      "avg / total       0.61      0.57      0.56       880\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.52231296,  0.51771904,  0.51920321,  0.5269497 ,  0.53510422,\n",
       "        0.53976703,  0.51430653,  0.52304192,  0.50787193,  0.5573232 ])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(simple_pipe, docs_list, labels_list, cv=10, scoring=make_scorer(fancy_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_cleaner = TransformerABC(prefilter_substitutions=['strip', 'lower'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_tokenizer(s):\n",
    "    return s.split()\n",
    "\n",
    "def lemmas_tokenizer(s):\n",
    "    return [s.lemma_ for s in nlp(s)]\n",
    "\n",
    "def lemmas_no_punct_tokenizer(s):\n",
    "    return [s.lemma_ for s in nlp(s) if not any([s.is_punct, s.is_space])]\n",
    "\n",
    "def lemmas_no_punt_no_num(s):\n",
    "    return [s.lemma_ for s in nlp(s) if not any([s.is_punct, s.is_space, s.is_digit, s.is_like_num])]\n",
    "\n",
    "def lemmas_sub_nums(s):\n",
    "    pass\n",
    "\n",
    "def lemmas_no_punct_sub_nums():\n",
    "    pass\n",
    "\n",
    "# Spacy tokenization, no lemmatization..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sk_stops = ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(preprocessor=simple_cleaner)),\n",
    "        ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "sgd_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(preprocessor=simple_cleaner)),\n",
    "        ('clf', SGDClassifier())\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(preprocessor=simple_cleaner)),\n",
    "        ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "svm_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(preprocessor=simple_cleaner)),\n",
    "        ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "# TODO:\n",
    "# - TFIDF\n",
    "# - log-count ratio?\n",
    "# - binarize\n",
    "\n",
    "pipes = [mnb_pipe, sgd_pipe, rf_pipe, svm_pipe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_grid_params = {\n",
    "    'vect__ngram_range': ((1,1),(1,2),(1,3)),\n",
    "    'vect__stop_words': ('english', None),\n",
    "    'vect__tokenizer': (simple_tokenizer, lemmas_tokenizer, lemmas_no_punct_tokenizer),\n",
    "    'clf__alpha': (0.001, 0.01, 0.1, 1)\n",
    "}\n",
    "\n",
    "sgd_grid_params = {\n",
    "    'vect__ngram_range': ((1,1),(1,2),(1,3)),\n",
    "    'vect__stop_words': ('english', None),\n",
    "    'vect__tokenizer': (lemmas_tokenizer, lemmas_no_punct_tokenizer),\n",
    "    'clf__loss': ('hinge', 'log', 'perceptron'),\n",
    "    'clf__penalty': ('l1', 'l2', 'elasticnet'),\n",
    "    'clf__alpha': (0.0001, 0.00001, 0.000001),\n",
    "    'clf__l1_ratio': (0.15, 0.05, 0.005),\n",
    "    'clf__fit_intercept': (True, False),\n",
    "    'clf__n_iter': (10, 50, 85)\n",
    "}\n",
    "\n",
    "rf_grid_params = {}\n",
    "\n",
    "svm_grid_params = {}\n",
    "\n",
    "grids = [mnb_grid_params, sgd_grid_params, rf_grid_params, svm_grid_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "fmt = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hndlr = logging.StreamHandler()\n",
    "hndlr.setFormatter(fmt)\n",
    "logger.addHandler(hndlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_grid(pipe, params, data, labels):\n",
    "    grid_search = GridSearchCV(pipe, params, n_jobs=-1, scoring=make_scorer(f1_weighted_scorer), cv=3, verbose=1)\n",
    "    \n",
    "    print \"Performing grid search...\"\n",
    "    print \"pipeline:\", [name for name, _ in pipe.steps]\n",
    "    print \"parameters:\"\n",
    "    pprint(params)\n",
    "    \n",
    "    t0 = time()\n",
    "    grid_search.fit(data, labels)\n",
    "    \n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "    print ''\n",
    "    \n",
    "    print \"Best score: %0.3f\" % grid_search.best_score_\n",
    "    print \"Best parameters set:\"\n",
    "    \n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()):\n",
    "        print \"\\t%s: %r\" % (param_name, best_parameters[param_name])\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.001, 0.01, 0.1, 1),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
      " 'vect__stop_words': ('english', None),\n",
      " 'vect__tokenizer': (<function simple_tokenizer at 0x7f5254a35b90>,\n",
      "                     <function lemmas_tokenizer at 0x7f5254a35f50>,\n",
      "                     <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>)}\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   54.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 343.143s\n",
      "\n",
      "Best score: 0.634\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__stop_words: None\n",
      "\tvect__tokenizer: <function lemmas_tokenizer at 0x7f5254a35f50>\n"
     ]
    }
   ],
   "source": [
    "mnb_grid = process_grid(mnb_pipe, mnb_grid_params, docs_list, labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.48942, std: 0.00521, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.50824, std: 0.00670, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.50508, std: 0.00504, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.50453, std: 0.00474, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.52849, std: 0.00333, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.52745, std: 0.00589, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.53247, std: 0.00492, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.54907, std: 0.00754, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.55504, std: 0.00831, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.58753, std: 0.00237, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.60793, std: 0.00317, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.60296, std: 0.00562, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.53292, std: 0.00350, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.54406, std: 0.00859, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.55488, std: 0.00663, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.001},\n",
       " mean: 0.59263, std: 0.00308, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.62247, std: 0.00773, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.61508, std: 0.00443, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.001},\n",
       " mean: 0.49806, std: 0.00343, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.51535, std: 0.00709, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.51180, std: 0.00525, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.51934, std: 0.00334, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.54093, std: 0.00389, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.53907, std: 0.00446, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.53851, std: 0.00585, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.55860, std: 0.00945, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.56025, std: 0.00923, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.59495, std: 0.00381, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.61818, std: 0.00467, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.61331, std: 0.00467, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.53928, std: 0.00623, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.55093, std: 0.00974, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.56115, std: 0.00577, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.01},\n",
       " mean: 0.60027, std: 0.00290, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.63066, std: 0.00851, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.62257, std: 0.00493, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.01},\n",
       " mean: 0.51106, std: 0.00105, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.52732, std: 0.00712, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.52160, std: 0.00438, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.53478, std: 0.00584, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.55604, std: 0.00495, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.55639, std: 0.00625, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.54672, std: 0.00760, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.56291, std: 0.00959, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.56512, std: 0.00947, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.59977, std: 0.00027, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.62333, std: 0.00555, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.62161, std: 0.00417, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.54829, std: 0.00826, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.55324, std: 0.00495, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.56679, std: 0.00731, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 0.1},\n",
       " mean: 0.59867, std: 0.00144, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.63438, std: 0.00746, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.62853, std: 0.00621, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 0.1},\n",
       " mean: 0.47899, std: 0.00304, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.48907, std: 0.00569, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.49457, std: 0.00437, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.45332, std: 0.00564, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.49761, std: 0.00234, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.50009, std: 0.00206, params: {'vect__ngram_range': (1, 1), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.48717, std: 0.01019, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.48016, std: 0.00682, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.50698, std: 0.00957, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.46347, std: 0.00666, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.49825, std: 0.00197, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.49807, std: 0.00569, params: {'vect__ngram_range': (1, 2), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.48898, std: 0.00843, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.47929, std: 0.00119, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.50588, std: 0.00801, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': 'english', 'clf__alpha': 1},\n",
       " mean: 0.46889, std: 0.00574, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function simple_tokenizer at 0x7f5254a35b90>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.50065, std: 0.00609, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_tokenizer at 0x7f5254a35f50>, 'vect__stop_words': None, 'clf__alpha': 1},\n",
       " mean: 0.50465, std: 0.00494, params: {'vect__ngram_range': (1, 3), 'vect__tokenizer': <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>, 'vect__stop_words': None, 'clf__alpha': 1}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_best.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       1.00      1.00      1.00       828\n",
      "     CARSON       1.00      1.00      1.00       644\n",
      "   CHRISTIE       1.00      1.00      1.00       616\n",
      "    CLINTON       1.00      1.00      1.00      1351\n",
      "       CRUZ       1.00      1.00      1.00      1009\n",
      "     KASICH       1.00      1.00      1.00       834\n",
      "      RUBIO       1.00      1.00      1.00      1214\n",
      "    SANDERS       1.00      1.00      1.00      1226\n",
      "      TRUMP       1.00      1.00      1.00      1129\n",
      "\n",
      "avg / total       1.00      1.00      1.00      8851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(labels_list, mnb_best.best_estimator_.predict(docs_list), target_names=candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_tuned_pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(preprocessor=simple_cleaner,\n",
    "                                 ngram_range=(1,3),\n",
    "                                 tokenizer=lemmas_tokenizer)),\n",
    "        ('clf', MultinomialNB(alpha=0.01))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.64      0.63      0.63        83\n",
      "     CARSON       0.60      0.55      0.58        65\n",
      "   CHRISTIE       0.50      0.47      0.48        62\n",
      "    CLINTON       0.69      0.79      0.73       136\n",
      "       CRUZ       0.73      0.61      0.67       101\n",
      "     KASICH       0.58      0.62      0.60        84\n",
      "      RUBIO       0.66      0.62      0.64       122\n",
      "    SANDERS       0.77      0.74      0.76       123\n",
      "      TRUMP       0.67      0.75      0.71       113\n",
      "\n",
      "avg / total       0.66      0.66      0.66       889\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.65      0.60      0.63        83\n",
      "     CARSON       0.55      0.51      0.53        65\n",
      "   CHRISTIE       0.59      0.60      0.59        62\n",
      "    CLINTON       0.64      0.73      0.68       135\n",
      "       CRUZ       0.73      0.68      0.70       101\n",
      "     KASICH       0.56      0.63      0.59        84\n",
      "      RUBIO       0.64      0.63      0.63       122\n",
      "    SANDERS       0.79      0.65      0.71       123\n",
      "      TRUMP       0.61      0.66      0.64       113\n",
      "\n",
      "avg / total       0.65      0.64      0.64       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.59      0.57      0.58        83\n",
      "     CARSON       0.51      0.54      0.52        65\n",
      "   CHRISTIE       0.45      0.48      0.47        62\n",
      "    CLINTON       0.68      0.76      0.72       135\n",
      "       CRUZ       0.77      0.62      0.69       101\n",
      "     KASICH       0.54      0.58      0.56        84\n",
      "      RUBIO       0.67      0.65      0.66       122\n",
      "    SANDERS       0.73      0.65      0.69       123\n",
      "      TRUMP       0.65      0.71      0.68       113\n",
      "\n",
      "avg / total       0.64      0.64      0.64       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.70      0.60      0.65        83\n",
      "     CARSON       0.68      0.46      0.55        65\n",
      "   CHRISTIE       0.48      0.56      0.52        62\n",
      "    CLINTON       0.62      0.62      0.62       135\n",
      "       CRUZ       0.76      0.68      0.72       101\n",
      "     KASICH       0.54      0.58      0.56        84\n",
      "      RUBIO       0.66      0.72      0.69       122\n",
      "    SANDERS       0.67      0.67      0.67       123\n",
      "      TRUMP       0.71      0.79      0.74       113\n",
      "\n",
      "avg / total       0.65      0.65      0.65       888\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.70      0.61      0.65        83\n",
      "     CARSON       0.61      0.69      0.65        64\n",
      "   CHRISTIE       0.68      0.58      0.63        62\n",
      "    CLINTON       0.68      0.70      0.69       135\n",
      "       CRUZ       0.75      0.71      0.73       101\n",
      "     KASICH       0.61      0.59      0.60        83\n",
      "      RUBIO       0.64      0.62      0.63       121\n",
      "    SANDERS       0.75      0.79      0.77       123\n",
      "      TRUMP       0.74      0.81      0.77       113\n",
      "\n",
      "avg / total       0.69      0.69      0.69       885\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.72      0.57      0.64        83\n",
      "     CARSON       0.47      0.38      0.42        64\n",
      "   CHRISTIE       0.58      0.55      0.56        62\n",
      "    CLINTON       0.66      0.68      0.67       135\n",
      "       CRUZ       0.77      0.71      0.74       101\n",
      "     KASICH       0.62      0.58      0.60        83\n",
      "      RUBIO       0.63      0.64      0.63       121\n",
      "    SANDERS       0.72      0.78      0.75       123\n",
      "      TRUMP       0.65      0.82      0.72       113\n",
      "\n",
      "avg / total       0.66      0.66      0.66       885\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.64      0.57      0.60        83\n",
      "     CARSON       0.50      0.56      0.53        64\n",
      "   CHRISTIE       0.47      0.43      0.45        61\n",
      "    CLINTON       0.67      0.74      0.70       135\n",
      "       CRUZ       0.73      0.73      0.73       101\n",
      "     KASICH       0.64      0.57      0.60        83\n",
      "      RUBIO       0.69      0.69      0.69       121\n",
      "    SANDERS       0.75      0.70      0.73       122\n",
      "      TRUMP       0.67      0.73      0.70       113\n",
      "\n",
      "avg / total       0.66      0.66      0.66       883\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.71      0.57      0.63        83\n",
      "     CARSON       0.54      0.50      0.52        64\n",
      "   CHRISTIE       0.62      0.56      0.59        61\n",
      "    CLINTON       0.65      0.64      0.65       135\n",
      "       CRUZ       0.75      0.65      0.70       101\n",
      "     KASICH       0.64      0.71      0.67        83\n",
      "      RUBIO       0.70      0.61      0.65       121\n",
      "    SANDERS       0.71      0.74      0.72       122\n",
      "      TRUMP       0.63      0.87      0.73       113\n",
      "\n",
      "avg / total       0.67      0.66      0.66       883\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.71      0.59      0.64        82\n",
      "     CARSON       0.56      0.53      0.54        64\n",
      "   CHRISTIE       0.61      0.59      0.60        61\n",
      "    CLINTON       0.61      0.66      0.64       135\n",
      "       CRUZ       0.80      0.73      0.76       101\n",
      "     KASICH       0.53      0.53      0.53        83\n",
      "      RUBIO       0.70      0.72      0.71       121\n",
      "    SANDERS       0.78      0.73      0.75       122\n",
      "      TRUMP       0.66      0.79      0.72       113\n",
      "\n",
      "avg / total       0.67      0.67      0.67       882\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       BUSH       0.70      0.59      0.64        82\n",
      "     CARSON       0.49      0.44      0.46        64\n",
      "   CHRISTIE       0.60      0.59      0.60        61\n",
      "    CLINTON       0.66      0.72      0.69       135\n",
      "       CRUZ       0.79      0.68      0.73       100\n",
      "     KASICH       0.60      0.61      0.61        83\n",
      "      RUBIO       0.62      0.66      0.64       121\n",
      "    SANDERS       0.80      0.78      0.79       122\n",
      "      TRUMP       0.68      0.77      0.72       112\n",
      "\n",
      "avg / total       0.67      0.67      0.67       880\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.66223059,  0.64484832,  0.63723762,  0.64807637,  0.68796454,\n",
       "        0.65502752,  0.65799286,  0.66213803,  0.66871257,  0.66857936])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mnb_tuned_pipe, docs_list, labels_list, cv=10, scoring=make_scorer(fancy_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6592807780000001"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ 0.66223059,  0.64484832,  0.63723762,  0.64807637,  0.68796454,\n",
    "        0.65502752,  0.65799286,  0.66213803,  0.66871257,  0.66857936])/10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.0001, 1e-05, 1e-06),\n",
      " 'clf__fit_intercept': (True, False),\n",
      " 'clf__l1_ratio': (0.15, 0.05, 0.005),\n",
      " 'clf__loss': ('hinge', 'log', 'perceptron'),\n",
      " 'clf__n_iter': (10, 50, 85),\n",
      " 'clf__penalty': ('l1', 'l2', 'elasticnet'),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
      " 'vect__stop_words': ('english', None),\n",
      " 'vect__tokenizer': (<function lemmas_tokenizer at 0x7f52783ee848>,\n",
      "                     <function lemmas_no_punct_tokenizer at 0x7f5254e21b18>)}\n",
      "Fitting 3 folds for each of 5832 candidates, totalling 17496 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 30.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 48.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 69.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 94.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 123.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 155.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 192.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 232.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 276.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 324.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 376.2min\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed: 431.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed: 491.3min\n",
      "[Parallel(n_jobs=-1)]: Done 14434 tasks      | elapsed: 554.8min\n",
      "[Parallel(n_jobs=-1)]: Done 16184 tasks      | elapsed: 622.4min\n",
      "[Parallel(n_jobs=-1)]: Done 17496 out of 17496 | elapsed: 672.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 40381.515s\n",
      "\n",
      "Best score: 0.579\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.0001\n",
      "\tclf__fit_intercept: False\n",
      "\tclf__l1_ratio: 0.05\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__n_iter: 85\n",
      "\tclf__penalty: 'l2'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__stop_words: None\n",
      "\tvect__tokenizer: <function lemmas_no_punct_tokenizer at 0x7f5254e21b18>\n"
     ]
    }
   ],
   "source": [
    "sgd_grid = process_grid(sgd_pipe, sgd_grid_params, docs_list, labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.001, 0.01, 0.1, 1),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
      " 'vect__stop_words': ('english', None),\n",
      " 'vect__tokenizer': (<function simple_tokenizer at 0x7f5254a35b90>,\n",
      "                     <function lemmas_tokenizer at 0x7f5254a35f50>,\n",
      "                     <function lemmas_no_punct_tokenizer at 0x7f5254a35ed8>)}\n",
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:  5.0min\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed: 23.6min\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed: 54.2min\n",
      "[Parallel(n_jobs=1)]: Done 720 out of 720 | elapsed: 89.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5363.179s\n",
      "\n",
      "Best score: 0.659\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__stop_words: None\n",
      "\tvect__tokenizer: <function lemmas_tokenizer at 0x7f5254a35f50>\n"
     ]
    }
   ],
   "source": [
    "process_grid(mnb_pipe, mnb_grid_params, docs_list, labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt = u\"Listening to this, do you think this is the tone  this immigration debate that republicans need to take to win back Hispanics into our party especially states like where we are in Nevada that has a pretty Hispanic community?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'listen | to | this | , | do | you | think | this | be | the | tone | -- | this | immigration | debate | that | republicans | need | to | take | to | win | back | hispanic | into | our | party | especially | state | like | where | we | be | in | nevada | that | have | a | pretty | hispanic | community | ?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = nlp(txt)\n",
    "' | '.join(tok.lemma_ for tok in toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'--' in set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks[11].is_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
